# Capstone Project: Explainability and Interpretability of LLMs

## Project Overview
This project focuses on enhancing trust in Large Language Models (LLMs) by implementing techniques like attention visualization, Layer-wise Relevance Propagation (LRP), and saliency maps.

## Repository Structure
- `notebooks/`: Colab notebooks for experiments and analysis.
- `documents/`: Supporting references and documentation.
- `data/`: Datasets and scripts for data preparation.
- `src/`: Python scripts for LLM interpretability.
- `images/`: Visualizations and diagrams.

## Getting Started
1. Clone the repository:
